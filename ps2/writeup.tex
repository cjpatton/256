 \documentclass[12pt]{article}
\usepackage{url,graphicx,tabularx,array,geometry,amsmath,tikz}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{listings}
\usetikzlibrary{arrows}
\setlength{\parskip}{1ex} %--skip lines between paragraphs
\setlength{\parindent}{0pt} %--don't indent paragraphs
\newenvironment{myindentpar}[1]% %indent whole paragraph when needed
 {\begin{list}{}%
         {\setlength{\leftmargin}{#1}}%
         \item[]%
 }
 {\end{list}}
%-- Commands for header
\renewcommand{\title}[1]{\textbf{#1}\\}
\renewcommand{\line}{\begin{tabularx}{\textwidth}{X>{\raggedleft}X}\hline\\\end{tabularx}\\[-0.5cm]}
\newcommand{\leftright}[2]{\begin{tabularx}{\textwidth}{X>{\raggedleft}X}#1%
& #2\\\end{tabularx}\\[-0.5cm]}
%\linespread{2} %-- Uncomment for Double Space
\begin{document}

\title{ECS 256 - Problem set 2}
\line
Olga Prilepova, Christopher Patton, Alexander Rumbaugh, John Chen, Thomas Provan

\textbf{Problem 1.a}\\
A coin is flipped $k$ times with $p$ probability of heads. For each head, the coin is flipped one additional time (a bonus flip). The number of bonus flips is referred to as $Y$ and the total number of heads $X$\newline
\newline
Var(X) can be found using the Law of Total Variance, and properties of binomial distributions. We will also need to use part of the derivation of EX:
\begin{equation*}
	\begin{aligned}
	E(X|Y)&= E({X - Y} + Y | Y)\\
	  &= E((X-Y)|Y) + E(Y|Y) &(\textrm{by 3.13})\\
	  &= pY + Y &(\textrm{by 3.110})\\
	  &= (1+p)Y\\
	  \\
	Var(X)	&= E[Var(X|Y)] + Var[E(X|Y)]	&(\textrm{by 9.8})\\
			&= E[Var(X|Y)] + Var[(1+p)Y]	&(\textrm{from above})\\
			&= E[Var(X|Y)] + (1+p)^{2}kp(1-p) &(\textrm{by 3.34 and 3.109})\\
			&= E[Yp(l-p)]  + (1+p)^{2}kp(1-p) &(\textrm{by 3.111})\\
			&= kp^{2}(1-p) + (1+p)^{2}kp(1-p) &(\textrm{by 3.103}) \\
			&= kp(1-p)\left(p+(1+p)^2\right)\\
			\textrm{Using p=0.5} \\
			&= k(0.25)(0.5+(1.5^{2}))\\
			&= 0.6875k\\
	\end{aligned}
\end{equation*}

\textbf{Problem 1.b}\\
In the trapped miner example, a miner chooses between three doors with only one leading to safety after $2$ hours. The other two doors lead back to the door room after $3$ and $5$ hours respectively.\newline
\newline
We are interesting the variance of $Y$, the time it takes to escape the mine. We will build upon Ahmed Ahmedin's solution to EY, where N refers to the total attempts needed to escape and $U_{i}$ refers to the time spent traveling on the $i^{th}$ attempt. 
\begin{equation*}
	\begin{aligned}
	Var(Y)	&= E[Var(Y|N)] + Var[E(Y|N)]	&(\textrm{by 9.8})\\
			&= E[Var(Y|N)] + Var[4N-2]	&(\textrm{by 9.16})\\
			&= E[Var(Y|N)] + 16Var[N]	&(\textrm{by 3.34 and 3.41})\\
			&= E[Var(Y|N)] + 16\cdot\frac{1-1/3}{(1/3)^{2}}	&(\textrm{by 3.93})\\
			&= E[Var(U_{1} + U_{2} + ... + U_{n} |N)] + 96\\
			&= E[Var(U_{1}|N) + ... + Var(U_{N-1}|N + Var(U_{N}|N)] + 96 &(\textrm{by 3.51})\\
			&= E[ 1 + 1 + ... 1 + 0 ] + 96\\
			&= E[N-1] + 96\\
			&= E[N] - 1 + 96	&(\textrm{by 3.17})\\
			&= 3 - 1 + 96	&(\textrm{by 3.92})\\
			&= 98\\
	\end{aligned}
\end{equation*}
We know that Var($U_{i}$|N) is independent because the miner's choice of door does not depend of a previous choice. Since we are conditioning this event on there being N attempts, the values of the first N-1 attempts will either be 3 or 5. So the variance of an individual attempt in this case is 1. The variance of the $N^{th}$ attempt is 0 because that attempt always is the same tunnel.

\pagebreak
\textbf{Problem 2.a}\\
For a vector $Q$ of random variables $(Q_1,..Q_n)$ we have:
\begin{equation*}
	\begin{aligned}
	Cov(Q)	&= E(QQ') - E(Q)E(Q') &(\textrm{by 13.53})\\
	\end{aligned}
\end{equation*}

Let $Q=Y|X$, where $Y$ is vector valued. 
Then:
\begin{equation*}
	\begin{aligned}
	Cov(Y|X) &= E\big((Y|X)(Y|X)'\big) - E(Y|X)E(Y|X)' &(\textrm{by 13.53})\\
	\end{aligned}
\end{equation*}

Taking expected value of both sides we have:
\begin{equation*}
	\begin{aligned}
	E\big(Cov(Y|X)\big) &= E\Big(E\big((Y|X)(Y|X)'\big) - E(Y|X)E(Y|X)'\Big)\\
	              &= E\Big(E\big((Y|X)(Y|X)'\big)\Big) - E\Big(E(Y|X)E(Y|X)'\Big)\\
	              &= E(YY') - E\Big(E(Y|X)E(Y|X)'\Big) &(\textrm{by Law of Tot. Expect.})\\
	\end{aligned}
\end{equation*}

Now let $Q=E(Y|X)$, where $Y$ is vector valued. 
Then:
\begin{equation*}
	\begin{aligned}
	Cov(E(Y|X)) &= E\big(E(Y|X)E(Y|X)'\big) - E\big(E(Y|X)\big)E\big(E(Y|X)\big)' &(\textrm{by 13.53})\\
	              &= E\big(E(Y|X)E(Y|X)'\big) - E(Y)E(Y)' &(\textrm{by Law of Tot. Expect.})\\
	\end{aligned}
\end{equation*}

Summing up the left sides and the right sides of these 2 equations we get:

\begin{equation*}
	\begin{aligned}
	E\big(Cov(Y|X)\big) + Cov(E(Y|X)) &= E(YY') - E\Big(E(Y|X)E(Y|X)'\Big) \\&+ E\big(E(Y|X)E(Y|X)'\big) - E(Y)E(Y)'\\
	E\big(Cov(Y|X)\big) + Cov(E(Y|X)) &= E(YY') - E(Y)E(Y)' \\
	E\big(Cov(Y|X)\big) + Cov(E(Y|X)) &= Cov(Y) &(\textrm{by 13.53})\\
	\end{aligned}
\end{equation*}


\textbf{Problem 2.b} 

First, just an equation to remind us of what we're actually trying to find here, the correlation between $X$ and $Y$.

\begin{equation*}
	\begin{aligned}
	\rho(X,Y) &=& \frac{Cov(X, Y)}{\sqrt{Var(X)}\sqrt{Var(Y)}} \\
	\end{aligned}
\end{equation*}

From the problem \textbf{2.a}, we have the following.

Let $B=X-Y$ and use that random variable in the following computations.

\begin{equation*}
	\begin{aligned}
	Cov(W) &= E\big(Cov(W|Z)\big) + Cov\big(E(W|Z)\big)\\
	\end{aligned}
\end{equation*}

Now we'll allow $W=(B,Y)'$ and $Z=Y$ in order to calculate $Cov(B,Y)'$.

\begin{equation*}
	\begin{aligned}
	Cov(B,Y) &= E\big[Cov(B,Y|Y)\big] + Cov\big[E(B,Y|Y)\big]\\
	\end{aligned}
\end{equation*}

\begin{equation*}
	\begin{aligned}
	Var(B + Y) &= Var(B) + Var(Y) + 2Cov(B,Y) \\
	Cov(B,Y) &= \big(Var(B + Y) - Var(B) - Var(Y)\big)/2 \\
	\end{aligned}
\end{equation*}

\begin{equation*}
	\begin{aligned}
	Cov\big(B,Y|Y\big) &= \big(Var(B + Y|Y) - Var(B|Y) - Var(Y|Y)\big)/2 \\
	Var(B + Y|Y) &= Var(B|Y)\\
	Var(Y|Y)&=0 \\
	Cov\big(B,Y|Y\big) &= 0 \\
	\end{aligned}
\end{equation*}

\begin{equation*}
	\begin{aligned}
	Cov\big(E(B,Y|Y)\big) &= \big(Var(EB + EY|Y) - Var(EB|Y) - Var(EY|Y)\big)/2 \\
	        &= \big(Var(1.5Y) - Var(0.5Y) - Var(Y)\big)/2 \\
	        &= \big(2.25Var(Y) - 0.25Var(Y) - Var(Y)\big)/2 \\
	        &= Var(Y) \\
	\end{aligned}
\end{equation*}

Hence:

\begin{equation*}
	\begin{aligned}
	Cov\big(B,Y\big) &= 0 + Var(Y)\\
	&= Var(Y)\\
	&= kp(1-p)\\
	&= 0.25k\\
	\end{aligned}
\end{equation*}

Now, let's see how we can get $Cov\big(X,Y\big)$ using that fact that we now know $Cov\big(B,Y\big)$. Remember, $X=B+Y$
\begin{equation*}
	\begin{aligned}
	Cov\big(X,Y\big) &= Cov\big(B+Y,Y\big)\\
	Cov\big(B+Y,Y\big) &= Cov\big(B,Y\big) + Cov\big(Y,Y\big)&(\textrm{13.2})\\
	&= Cov\big(B,Y\big) + Var(Y)\\
	\end{aligned}
\end{equation*}

Now we have all the ingredients to find the correlation:


\begin{equation*}
	\begin{aligned}
	\rho(X,Y) &= \frac{Cov(X, Y)}{\sqrt{Var(X)}\sqrt{Var(Y)}} \\
	       &= \frac{Cov\big(B,Y\big) + Var(Y)}{\sqrt{Var(X)}\sqrt{Var(Y)}} \\
	       &= \frac{0.25k + 0.25k}{\sqrt{0.6875k}\sqrt{0.25k}} \\
	       &= \frac{0.5k}{k\sqrt{0.6875}\sqrt{0.25}} \\
	       &= \frac{0.5}{\sqrt{0.6875*0.25}} \\
	       &= 1.2\\
	\end{aligned}
\end{equation*}


\pagebreak
\textbf{Problem 3} 
Let $X_i$ denote the state of the machine at time index $i$, where $X_0  
\sim \Pi$, the stationary distributions of the states. Finally, suppose there are $n$ states, 
named numerically 1 to $n$. We want to compute
$$\rho(X_{i+j}, X_i) = \frac{\text{Cov}(X_{i+j}, X_i)}
                 {\sqrt{\text{Var}(X_{i+j})} \cdot \sqrt{\text{Var}(X_i)}} $$ 
for all $1 \le j \le k$ and some $k$. We can simplify the denomonator by realizing 
that, since $X_i$ and $X_{i+j}$ are drawn from the same distribution, they have
the same variance. The equation becomes: 
$$\rho(X_{i+j}, X_i) = \frac{\text{Cov}(X_{i+j}, X_i)}
                 {\text{Var}(X_{i})}. $$
By definition, the expected value of $X_i$ is
$$ E(X_i) = \sum_{l=1}^n{l\cdot\pi_l}. $$ 
Thus, 
$$ \text{Var}(X_i) = E(X_i^2) - (EX_i)^2 =
  \sum_{l=1}^n{(l^2\pi_l)} - \Big[ \sum_{l=1}^n{l \pi_l} \Big]^2.$$ 

Now, let's derive $\text{Cov}(X_{i+j}, X_i)$. For this, we also need the expected
value of $X_{i+j}$:
$$ E(X_{i+j} | X_i) = \sum_{k=1}^n{k \cdot m^j_{l,k}} $$
where $M^j$ is the transition matrix $M \cdot M \cdots M$ ($j$ times), and 
$X_i = l$. 
$$ \text{Cov}(X_{i+j}, X_i) = E(X_{i+j}X_i) - E(X_{i+j})E(X_i) $$
Suppose $Q = X_{i,j}X_i$. By the law of total expectations, $ E(Q) = E(E(Q|X_i))$.
Thus, 
\begin{equation*}
  \begin{aligned}
    E(X_{i+j}X_i) &= \sum_{l=1}^n{ \pi_l l \cdot E(X_{i+j} | X_i)} \\
                  &= \sum_{l=1}^n{ \Bigg[ \pi_l l \cdot \Big[ 
                         \sum_{k=1}^n{k \cdot m^j_{l,k}} \Big]} \Bigg]. \\
  \end{aligned}
\end{equation*}
Since $X_{i+j}$ and $X_i$ have the same distribution ($\Pi$), the covariance becomes
$$ \text{Cov}(X_{i+j}, X_i) =  \sum_{l=1}^n{ \Bigg[ \pi_l l \cdot \Big[ 
                                     \sum_{k=1}^n{k \cdot m^j_{l,k}} \Big]} \Bigg]
                              - (EX_i)^2.$$

We can now write code to calculate the correlation of $X_i$ and $X_{i+j}$. The following
function \texttt{mccor(tm, k)} returns a vector corresponding to 
$\rho(X_i, X_{i+1}) \dots \rho(X_i, X_k)$. 

\lstset{
  basicstyle=\small,
  stringstyle=\ttfamily,
  numbers=left,
  numberstyle=\tiny,
  stepnumber=1, 
  numbersep=5pt,
  language=R }

\lstinputlisting{3pretty.R}

\end{document}
