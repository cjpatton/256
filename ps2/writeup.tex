 \documentclass[12pt]{article}
\usepackage{url,graphicx,tabularx,array,geometry,amsmath,tikz}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{listings}
\usetikzlibrary{arrows}
\setlength{\parskip}{1ex} %--skip lines between paragraphs
\setlength{\parindent}{0pt} %--don't indent paragraphs
\newenvironment{myindentpar}[1]% %indent whole paragraph when needed
 {\begin{list}{}%
         {\setlength{\leftmargin}{#1}}%
         \item[]%
 }
 {\end{list}}
%-- Commands for header
\renewcommand{\title}[1]{\textbf{#1}\\}
\renewcommand{\line}{\begin{tabularx}{\textwidth}{X>{\raggedleft}X}\hline\\\end{tabularx}\\[-0.5cm]}
\newcommand{\leftright}[2]{\begin{tabularx}{\textwidth}{X>{\raggedleft}X}#1%
& #2\\\end{tabularx}\\[-0.5cm]}
%\linespread{2} %-- Uncomment for Double Space
\begin{document}

\title{ECS 256 - Problem set 2}
\line
Olga Prilepova, Christopher Patton, Alexander Rumbaugh, John Chen, Thomas Provan

\textbf{Problem 1.a}\\
A coin is flipped $k$ times with $p$ probability of heads. For each head, the coin is flipped one additional time (a bonus flip). The number of bonus flips is referred to as $Y$ and the total number of heads $X$\newline
\newline
Var(X) can be found using the Law of Total Variance, and properties of binomial distributions. We will also need to use part of the derivation of EX:
\begin{equation*}
	\begin{aligned}
	E(X|Y)&= E({X - Y} + Y | Y)\\
	  &= E((X-Y)|Y) + E(Y|Y) &(\textrm{by 3.13})\\
	  &= pY + Y &(\textrm{by 3.110})\\
	  &= (1+p)Y\\
	  \\
	Var(X)	&= E[Var(X|Y)] + Var[E(X|Y)]	&(\textrm{by 9.8})\\
			&= E[Var(X|Y)] + Var[(1+p)Y]	&(\textrm{from above})\\
			&= E[Var(X|Y)] + (1+p)^{2}kp(1-p) &(\textrm{by 3.34 and 3.109})\\
			&= E[Yp(l-p)]  + (1+p)^{2}kp(1-p) &(\textrm{by 3.111})\\
			&= kp^{2}(1-p) + (1+p)^{2}kp(1-p) &(\textrm{by 3.103}) \\
			&= kp(1-p)\left(p+(1+p)^2\right)\\
			\textrm{Using p=0.5} \\
			&= k(0.25)(0.5+(1.5^{2}))\\
			&= 0.6875k\\
	\end{aligned}
\end{equation*}

\textbf{Problem 1.b}\\
In the trapped miner example, a miner chooses between three doors with only one leading to safety after $2$ hours. The other two doors lead back to the door room after $3$ and $5$ hours respectively.\newline
\newline
We are interesting the variance of $Y$, the time it takes to escape the mine. We will build upon Ahmed Ahmedin's solution to EY, where N refers to the total attempts needed to escape and $U_{i}$ refers to the time spent traveling on the $i^{th}$ attempt. 
\begin{equation*}
	\begin{aligned}
	Var(Y)	&= E[Var(Y|N)] + Var[E(Y|N)]	&(\textrm{by 9.8})\\
			&= E[Var(Y|N)] + Var[4N-2]	&(\textrm{by 9.16})\\
			&= E[Var(Y|N)] + 16Var[N]	&(\textrm{by 3.34 and 3.41})\\
			&= E[Var(Y|N)] + 16\cdot\frac{1-1/3}{(1/3)^{2}}	&(\textrm{by 3.93})\\
			&= E[Var(U_{1} + U_{2} + ... + U_{n} |N)] + 96\\
			&= E[Var(U_{1}|N) + ... + Var(U_{N-1}|N + Var(U_{N}|N)] + 96 &(\textrm{by 3.51})\\
			&= E[ 1 + 1 + ... 1 + 0 ] + 96\\
			&= E[N-1] + 96\\
			&= E[N] - 1 + 96	&(\textrm{by 3.17})\\
			&= 3 - 1 + 96	&(\textrm{by 3.92})\\
			&= 98\\
	\end{aligned}
\end{equation*}
We know that Var($U_{i}$|N) is independent because the miner's choice of door does not depend of a previous choice. Since we are conditioning this event on there being N attempts, the values of the first N-1 attempts will either be 3 or 5. So the variance of an individual attempt in this case is 1. The variance of the $N^{th}$ attempt is 0 because that attempt always is the same tunnel.

\pagebreak
\textbf{Problem 2.a}\\
For a vector $Q$ of random variables $(Q_1,..Q_n)$ we have:
\begin{equation*}
	\begin{aligned}
	Cov(Q)	&= E(QQ') - E(Q)E(Q') &(\textrm{by 13.53})\\
	\end{aligned}
\end{equation*}

Let $Q=Y|X$, where $Y$ is vector valued. 
Then:
\begin{equation*}
	\begin{aligned}
	Cov(Y|X) &= E\big((Y|X)(Y|X)'\big) - E(Y|X)E(Y|X)' &(\textrm{by 13.53})\\
	\end{aligned}
\end{equation*}

Taking expected value of both sides we have:
\begin{equation*}
	\begin{aligned}
	E\big(Cov(Y|X)\big) &= E\Big(E\big((Y|X)(Y|X)'\big) - E(Y|X)E(Y|X)'\Big)\\
	              &= E\Big(E\big((Y|X)(Y|X)'\big)\Big) - E\Big(E(Y|X)E(Y|X)'\Big)\\
	              &= E(YY') - E\Big(E(Y|X)E(Y|X)'\Big) &(\textrm{by Law of Tot. Expect.})\\
	\end{aligned}
\end{equation*}

Now let $Q=E(Y|X)$, where $Y$ is vector valued. 
Then:
\begin{equation*}
	\begin{aligned}
	Cov(E(Y|X)) &= E\big(E(Y|X)E(Y|X)'\big) - E\big(E(Y|X)\big)E\big(E(Y|X)\big)' &(\textrm{by 13.53})\\
	              &= E\big(E(Y|X)E(Y|X)'\big) - E(Y)E(Y)' &(\textrm{by Law of Tot. Expect.})\\
	\end{aligned}
\end{equation*}

Summing up the left sides and the right sides of these 2 equations we get:

\begin{equation*}
	\begin{aligned}
	E\big(Cov(Y|X)\big) + Cov(E(Y|X)) &= E(YY') - E\Big(E(Y|X)E(Y|X)'\Big) \\&+ E\big(E(Y|X)E(Y|X)'\big) - E(Y)E(Y)'\\
	E\big(Cov(Y|X)\big) + Cov(E(Y|X)) &= E(YY') - E(Y)E(Y)' \\
	E\big(Cov(Y|X)\big) + Cov(E(Y|X)) &= Cov(Y) &(\textrm{by 13.53})\\
	\end{aligned}
\end{equation*}


\textbf{Problem 2.b} 

TODO. 

First, just an equation to remind us of what we're actually trying to find here, the correlation between $X$ and $Y$.

\begin{equation*}
	\begin{aligned}
	\rho(X,Y) &=& \frac{Cov(X, Y)}{\sqrt{Var(X)}\sqrt{Var(Y)}} \\
	\end{aligned}
\end{equation*}

But $X$ is too complicated and annoying to work with, so we're going to change the random vector to find the correlation matrix of from $(X,Y)'$ to $(B,Y)'$, where $B$ is the random variable that expresses the number of bonus heads flipped, via eq. 13.54. Then we'll expand that forumla out so we know what the final equation we need to solve for is.

\begin{equation*}
	\begin{aligned}\left(\begin{array}{c}
			X\\
			Y
		\end{array}\right) & = \left(\begin{array}{c}
			B+Y\\
			Y
		\end{array}\right)\\
 		& = \left(\begin{array}{cc}
			1 & 1\\
			0 & 1
		\end{array}\right)\left(\begin{array}{c}
			B\\
			Y
		\end{array}\right)
	\end{aligned}
\end{equation*}

\begin{equation*}
	\begin{aligned}
	Cov\big[(X,Y)'\big] &= \left(
		\begin{array}{cc}
			Var(X) & Cov(X,Y) \\
			Cov(X,Y) & Var(Y)
		\end{array}
	\right)\\
	&= \left(
		\begin{array}{cc}
			1 & 1\\
			0 & 1
		\end{array}
		\right)
		Cov\big[(B,Y)'\big]
		\left(
		\begin{array}{cc}
			1 & 0\\
			1 & 1
		\end{array}
		\right) & (\textrm{By 13.54}) \\
	&= \left(
		\begin{array}{cc}
			1 & 1\\
			0 & 1
		\end{array}
		\right)
		\left(
		\begin{array}{cc}
			Var(B) & Cov(B,Y) \\
			Cov(B,Y) & Var(Y)
		\end{array}
		\right)
		\left(
		\begin{array}{cc}
			1 & 0\\
			1 & 1
		\end{array}
		\right) \\
	&= \left(
		\begin{array}{cc}
			Var(B) + Cov(B,Y) & Cov(B,Y) + Var(Y)\\
			Cov(B,Y) & Var(Y)
		\end{array}
		\right)
		\left(
		\begin{array}{cc}
			1 & 0\\
			1 & 1
		\end{array}
		\right) \\
	&= \left(
		\begin{array}{cc}
			Var(B) + Var(Y) + 2Cov(B,Y) & Cov(B,Y) + Var(Y)\\
			Cov(B,Y) + Var(Y) & Var(Y)
		\end{array}
		\right)
	\end{aligned}
\end{equation*}

Now that we know the final form, we'll go back and calculate $Cov[(B,Y)']$. From the problem \textbf{2.a}, we have the following.

\begin{equation*}
	\begin{aligned}
	Cov(W) &= E\big(Cov(W|Z)\big) + Cov\big(E(W|Z)\big)\\
	\end{aligned}
\end{equation*}

Now we'll allow $W=(B,Y)'$ and $Z=Y$ in order to calculate for $Cov\big[(B,Y)'\big]$.

\begin{equation*}
	\begin{aligned}
	Cov\big((B,Y)'\big) &= E\big[Cov((B,Y)'|Y)\big] + Cov\big[E((B,Y)'|Y)\big]\\
	\end{aligned}
\end{equation*}

Now we'll consider each half of that individually, then add them together. First, let's consider the random variable $Q_c = Cov\big((B,Y)'|Y\big)$

\begin{equation*}
	\begin{aligned}
		Cov\big((B,Y)' | Y\big) &=
		\begin{cases}
			\textrm{for $i$ from $0..k$} \\
			Cov((B,Y)'|Y=i)\: w.p.\binom{k}{i} 0.5^k
		\end{cases}
		\\
	\end{aligned}
\end{equation*}

\begin{equation*}
	\begin{aligned}
		Cov\big((B,Y)' | Y=i\big) &=
		 \left(
		\begin{array}{cc}
			Var(B|Y=i) & Cov(B,Y|Y=i)\\
			Cov(B,Y | Y=i) & Var(Y|Y=i)
		\end{array}
		\right) \\
		&=
		 \left(
		\begin{array}{cc}
			i/4 & Cov(B,Y|Y=i)\\
			Cov(B,Y | Y=i) & 0
		\end{array}
		\right) \\
	\end{aligned}
\end{equation*}


\begin{equation*}
	\begin{aligned}
		Cov\big(B,Y | Y=i\big) &=
		E(BY|Y=i) - E(B|Y=i) * E(Y|Y=i)\\
		& = E(iB|Y=i) - i * E(B|Y=1)\\
		& = i*E(B|Y=i) - i * E(B|Y=1)\\
		& = 0
	\end{aligned}
\end{equation*}

\begin{equation*}
	\begin{aligned}
		Cov\big((B,Y)' | Y=i\big) &=
		 \left(
		\begin{array}{cc}
			i/4 & 0\\
			0 & 0
		\end{array}
		\right) \\
	\end{aligned}
\end{equation*}

\begin{equation*}
	\begin{aligned}
		E\Big[Cov\big((B,Y)' | Y=i\big)\Big] &=
		\sum_{i=0}^{k} 
		\left(
		\begin{array}{cc}
			i/4 & 0\\
			0 & 0
		\end{array}
		\right) * \binom{k}{i} 0.5^k\\
	\end{aligned}
\end{equation*}

(( Should be able to reduce that, don't remember how. Do double check my work on those covariances. I am reasonably certain they all zero out, which would be greatand makes intuitive sense to me (the covariance of anything with a scalar, which (Y|y=i) is, is 0) but I'm not 100\% confident about it ))

Now then, onto the other half, $Cov\big[E((B,Y)'|Y)\big]$ First, that expected value part.

\begin{equation*}
	\begin{aligned}
		E\big((B,Y)' | Y\big) &=
		\begin{cases}
			\textrm{for $i$ from $0..k$} \\
			E((B,Y)'|Y=i)\: w.p.\binom{k}{i} 0.5^k
		\end{cases}
		\\
	\end{aligned}
\end{equation*}

\begin{equation*}
	\begin{aligned}
		E\big((B,Y)' | Y=i\big) &=
		 \left(
		\begin{array}{c}
			E(B|Y=i) \\
			E(Y|Y=i)
		\end{array}
		\right) \\
		&=
		 \left(
		\begin{array}{c}
			i/2 \\
			i
		\end{array}
		\right) \\
	\end{aligned}
\end{equation*}

(( Okay, here's the point at which I get stuck. We now need to get the covariance matrix of this. Hmm... Covariance is the hardest one by far. I think the two variances are pretty easy, fairly sure they fall out from the variance of the binomial. I'll write up the matrix form of what we need, hopefully Olga has made progress by the time I wake up.))

\begin{equation*}
	\begin{aligned}
		Cov\big[E((B,Y)' | Y)\big] &=
		 \left(
		\begin{array}{cc}
			Var(E(B|Y)) & Cov\big(E(B|Y),E(Y|Y)\big)\\
			Cov\big(E(B|Y),E(Y|Y)\big) & Var(E(Y|Y))
		\end{array}
		\right) \\
	\end{aligned}
\end{equation*}




\pagebreak
\textbf{Problem 3} 

$$\rho(X_{i+j}, X_i) = \frac{\text{Cov}(X_{i+j}, X_i)}
                 {\sqrt{\text{Var}(X_{i+j})} \cdot \sqrt{\text{Var}(X_i)}} $$ 

A couple things we noticed: 
$$ E(X_i) = \sum_{l=1}^n{l\cdot\pi_l} $$ 
$$ E(X_{i+j} | X_i) = \sum_{l=1}^n{l \cdot m^j_{k,l}} $$
where $M^j$ is the transition matrix $M \cdot M \cdots M$ ($j$ times), and 
$X_i = k$. First, let's derive $\text{Cov}(X_{i+j}, X_i)$. 
$$ \text{Cov}(X_{i+j}, X_i) = E(X_{i,j}X_i) - E(X_{i+j})E(X_i) $$
Suppose $Q = X_{i,j}X_i$. By the law of total expectations, $ E(Q) = E(E(Q|X_i))$.
Thus, 
\begin{equation*}
  \begin{aligned}
    E(X_{i,j}X_i) &= \sum_{l=1}^n{ \pi_l l \cdot E(X_{i+j} | X_i)} \\
                  &= \sum_{l=1}^n{ \Bigg[ \pi_l l \cdot \Big[ 
                         \sum_{k=1}^n{k \cdot m^j_{l,k}} \Big]} \Bigg]. \\
  \end{aligned}
\end{equation*}
Since $X_{i,j}$ and $X_i$ have the same distribution ($\Pi$), the covariance becomes
$$ \text{Cov}(X_{i+j}, X_i) =  \sum_{l=1}^n{ \Bigg[ \pi_l l \cdot \Big[ 
                                     \sum_{k=1}^n{k \cdot m^j_{l,k}} \Big]} \Bigg]
                              - (EX_i)^2.$$

Next, we derive $\text{Var}(X_{i,j})$ and $\text{Var}(X_i)$. \textbf{TODO}



\end{document}
